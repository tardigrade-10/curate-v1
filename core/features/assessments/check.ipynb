{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "curpath = os.getcwd()\n",
    "os.chdir(curpath.split(\"core\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "\n",
    "from core.features.assessments.prompts import SIMPLE_ASSIGNMENT_CHECK_PROMPT\n",
    "from core.features.utils import add_dicts, calculate_cost_gpt4_turbo\n",
    "from core.features.provider import creator, async_creator, text_model_defaults\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- marks = 6\n",
    "\n",
    "- feedback = you are good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[23, [34, 45]]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "a = [23, [34, 45]]\n",
    "\n",
    "obj = json.dumps(a)\n",
    "\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "q = \"#q1\\n\"\n",
    "\n",
    "q.replace('q', 'a')\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(i):\n",
    "        if j > 5:\n",
    "            print(j)\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def question_or_answer_cell(s):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to check the correct tag for a question or an answer cell.\n",
    "    Must match - \"#q<some + integer>\" or \"#a<some + integer>\"\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = r\"^#[aq]\\d+\"\n",
    "    s = s.strip()\n",
    "    return bool(re.match(pattern, s))\n",
    "\n",
    "\n",
    "def qna_extraction(file_path):\n",
    "    \"\"\"\n",
    "    Function to extracts question and answers from the jupyter notebook\n",
    "    cells with #qn tag and #an tag\n",
    "    return the dict of all the questions and answers \n",
    "    \"\"\"\n",
    "\n",
    "    if file_path.endswith(\".ipynb\"):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            nb = json.load(f)\n",
    "    else:\n",
    "        ValueError(\"File type not supported. Must be a valid Jupyter Notebook file.\")\n",
    "\n",
    "    cells = nb['cells']\n",
    "    qnas = {}\n",
    "    for cell in cells:\n",
    "        if question_or_answer_cell(cell['source'][0]):\n",
    "            qnas[cell['source'][0].strip('#\\n')] = \"\".join(cell['source'][1:])  \n",
    "    return qnas\n",
    "\n",
    "\n",
    "async def assessment_check(qna_dict):\n",
    "\n",
    "    \"\"\"\n",
    "    Main function to check the solution of the answer\n",
    "    provide obtained marks and feedback\n",
    "    \"\"\"\n",
    "\n",
    "    conversation = [{\"role\": \"system\", \"content\": SIMPLE_ASSIGNMENT_CHECK_PROMPT}]\n",
    "    user_input = f\"\"\"\n",
    "    \n",
    "    {qna_dict}\n",
    "\n",
    "    \"\"\"\n",
    "    user_message = {'role': 'user', 'content': user_input}\n",
    "    conversation.append(user_message)\n",
    "\n",
    "    response = await async_creator(\n",
    "        **text_model_defaults,\n",
    "        messages=conversation\n",
    "    )    \n",
    "    response = response.model_dump()\n",
    "    output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    total_usage = response[\"usage\"]\n",
    "    return {\"output\": output, \"total_usage\": total_usage}\n",
    "\n",
    "\n",
    "\n",
    "def notebook_check(qnas, marks):\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes the set of questions and answers and provide a dict with remarks of all questions\n",
    "    \"\"\"\n",
    "\n",
    "    total_usage = {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}\n",
    "    remarks = {}\n",
    "    for i in range(1, len(marks)+1):\n",
    "        qna_dict = {\n",
    "            \"question\": qnas[f\"q{i}\"],\n",
    "            \"solution\": qnas[f\"a{i}\"],\n",
    "            \"max_marks\": marks[f\"q{i}\"]\n",
    "        }\n",
    "        result = assessment_check(qna_dict=qna_dict)\n",
    "        output = json.loads(result['output'])\n",
    "        usage = result['total_usage']\n",
    "\n",
    "        total_usage = add_dicts(total_usage, usage)\n",
    "        \n",
    "        qna_dict.update(output)\n",
    "        remarks[f\"q{i}\"] = qna_dict\n",
    "\n",
    "    return remarks, total_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_assessment_check(path, marks=None, remarks_filename=\"remarks.json\"):\n",
    "    \"\"\"\n",
    "    This function is the main batch processing function for assessment\n",
    "    Takes path of folder or notebook, check all the questions of all the notebooks and provide a combined json for remarks\n",
    "\n",
    "    path: can be jupyter notebook path or a folder full of jupyter notebooks\n",
    "    marks: a dict with questions(qn) and max_marks mapping\n",
    "    \"\"\"\n",
    "    \n",
    "    total_tokens = {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}\n",
    "    final_remarks = {}\n",
    "\n",
    "    remarks_path = \"/\"\n",
    "    if os.path.isdir(path):\n",
    "        print(\"GOT A DIR\")\n",
    "        file_paths = [os.path.join(path, file_path) for file_path in os.listdir(path)[:2] if file_path.endswith('.ipynb')]\n",
    "        print(f\"GOT {len(file_paths)} notebooks\")\n",
    "        files_and_qnas = {file_path: qna_extraction(file_path) for file_path in file_paths}\n",
    "        print(\"EXTRACTED QNAS\")\n",
    "        remarks_path = os.path.join(path, 'remarks')\n",
    "        os.makedirs(remarks_path, exist_ok=True)\n",
    "        print(\"CREATED REMARKS FOLDER\")\n",
    "\n",
    "    elif path.endswith('.ipynb'):\n",
    "        print(\"GOT A NOTEBOOK\")\n",
    "        qnas = qna_extraction(file_path=path)\n",
    "        print(\"EXTRACTED QNAS\")\n",
    "        files_and_qnas = {path: qnas}\n",
    "        remarks_path = os.path.dirname(path)\n",
    "\n",
    "    for file, qna in tqdm(files_and_qnas.items()):\n",
    "        remarks, tokens = notebook_check(qna, marks)\n",
    "        # remarks_dict = {f\"q{i+1}\": remark for i, remark in enumerate(remarks)}\n",
    "        # print(remarks)\n",
    "        final_remarks[os.path.basename(file)] = remarks\n",
    "        total_tokens = add_dicts(total_tokens, tokens)\n",
    "\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    # saving the remarks\n",
    "    json_remarks = json.dumps(final_remarks, indent=4)\n",
    "    with open(os.path.join(remarks_path, remarks_filename), 'w') as f:\n",
    "        f.write(json_remarks)\n",
    "\n",
    "    print(\"DONE!\")\n",
    "\n",
    "    return final_remarks, total_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOT A DIR\n",
      "GOT 2 notebooks\n",
      "EXTRACTED QNAS\n",
      "CREATED REMARKS FOLDER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:05<00:00, 32.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING COMPLETE\n",
      "DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# assignment 1 test cases\n",
    "\n",
    "marks = {\n",
    "    \"q1\": 1,\n",
    "    \"q2\": 1,\n",
    "    \"q3\": 1,\n",
    "    \"q4\": 4,\n",
    "    \"q5\": 5,\n",
    "    \"q6\": 2,\n",
    "    \"q7\": 2,\n",
    "    \"q8\": 8,\n",
    "    \"q9\": 1\n",
    "}\n",
    "\n",
    "ass1_folder = r'core\\features\\assessments\\ass1_notebooks'\n",
    "final_remarks, total_tokens = batch_assessment_check(ass1_folder, marks = marks, remarks_filename='remarks.json')\n",
    "\n",
    "print(calculate_cost_gpt4_turbo(total_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = [2,3,4,5]\n",
    "# for i in l:\n",
    "#     print(i)\n",
    "#     l.insert(1, '2')\n",
    "#     print(l)\n",
    "\n",
    "# print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598eba1b-90bf-4427-a2e8-09efeda228d5\n",
      "3a8cc642-e35c-455f-9b3e-a1eb29eec638\n",
      "8f225da0-f0e4-40d0-963f-8e548456f1a7\n",
      "47eae615-7add-48cc-9bcc-7f7e6a53e8d6\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "print(str(uuid.uuid4()))\n",
    "print(str(uuid.uuid4()))\n",
    "print(str(uuid.uuid4()))\n",
    "print(str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOT A DIR\n",
      "GOT 2 notebooks\n",
      "EXTRACTED QNAS\n",
      "CREATED REMARKS FOLDER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:38<00:00, 19.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING COMPLETE\n",
      "DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# assignment 2 test cases\n",
    "\n",
    "marks = {\n",
    "    \"q1\": 3,\n",
    "    \"q2\": 5,\n",
    "    \"q3\": 10,\n",
    "    \"q4\": 5,\n",
    "    \"q5\": 2\n",
    "}\n",
    "\n",
    "ass2_folder = r'core\\features\\assessments\\ass2_notebooks'\n",
    "final_remarks, total_tokens = batch_assessment_check(ass2_folder, marks = marks, remarks_filename='remarks2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'usd': 0.10194, 'inr': 8.4865}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_cost_gpt4_turbo(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'{1: 2}': 2}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {str({1:2}): 2}\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curate-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
