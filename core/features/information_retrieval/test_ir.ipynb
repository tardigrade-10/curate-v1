{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information retrieval from text and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "curpath = os.getcwd()\n",
    "os.chdir(curpath.split(\"core\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "from prompts import SIMPLE_INFO_RETRIEVAL_TEXT_PROMPT, SIMPLE_INFO_RETRIEVAL_IMAGE_PROMPT\n",
    "from core.features.topic_segregation.utils import add_dicts, calculate_cost_gpt4_8k, calculate_cost_gpt4_turbo\n",
    "from core.features.provider import creator\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "text_model_defaults = {\"model\" : \"gpt-4-1106-preview\", \"temperature\" : 0.1, \"response_format\" : {\"type\": \"json_object\"}}\n",
    "vision_model_defaults = {\"model\" : \"gpt-4-vision-preview\", \"temperature\" : 0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def information_retrieval_from_text(raw_text: str, queries: List):\n",
    "\n",
    "    if len(raw_text) > 100000:\n",
    "        raise ValueError(\"Raw Text is too long. Should be less than 100000 characters\")\n",
    "    \n",
    "    if len(queries) > 0:\n",
    "        queries = {i+1: q for i, q in enumerate(queries)}\n",
    "    else:\n",
    "        raise ValueError(\"Atleast one query should be present\")\n",
    "\n",
    "    conversation = [{\"role\": \"system\", \"content\": SIMPLE_INFO_RETRIEVAL_TEXT_PROMPT}]\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    RAW_TEXT:\n",
    "\n",
    "    //raw_text//\n",
    "\n",
    "    {raw_text}\n",
    "\n",
    "    //raw_text//\n",
    "\n",
    "    Queries:\n",
    "\n",
    "    {queries}\n",
    "    \n",
    "    OUTPUT:\n",
    "    \"\"\"\n",
    "\n",
    "    user_message = {'role': 'user', \"content\": user_prompt}\n",
    "    conversation.append(user_message)\n",
    "\n",
    "    total_usage = {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}\n",
    "    response = creator(\n",
    "                    **text_model_defaults,\n",
    "                    messages = conversation,\n",
    "                    )\n",
    "    \n",
    "    total_usage = add_dicts(total_usage, dict(response.usage))\n",
    "\n",
    "    output = response.choices[0].message.content\n",
    "\n",
    "    return {\"output\": output, \"total_usage\": total_usage}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\n",
    "\n",
    "path = r\"C:\\Users\\DELL\\Documents\\Curate\\curate-v1\\core\\features\\information_retrieval\\test_news1.txt\"\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    raw_text = f.read().decode('utf-8')\n",
    " \n",
    "queries = [\"Major Dates with Events\"]\n",
    "\n",
    "\n",
    "\n",
    "res = information_retrieval_from_text(raw_text, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215827"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"queries\": [\n",
      "        {\n",
      "            \"query\": \"1\",\n",
      "            \"answers\": [\n",
      "                \"November 09, 2023 06:20 pm - Mandal Education Officers in Andhra Pradesh told to engage with local community\",\n",
      "                \"November 10, 2023 - IIT Kanpur, IIS Kanpur to Establish Laboratories and Develop curriculum\",\n",
      "                \"November 10, 2023 - K12 Education Market to Reach $525.7 Billion by 2031\",\n",
      "                \"November 10, 2023 - NC State College of Education to House the Educational Opportunities Program for Individuals With Intellectual Disabilities\",\n",
      "                \"November 10, 2023 - Clean California Transforms Blighted Vacant Lot into Nursery and Education Center in the Heart of San Francisco\",\n",
      "                \"November 10, 2023 - Four public secondary schools have been recognised in the WA Education Awards for the exceptional way they prepare students for life beyond school\"\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(res[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion_tokens': 200, 'total_tokens': 43490, 'prompt_tokens': 43290}\n",
      "0.4389\n"
     ]
    }
   ],
   "source": [
    "print(res[\"total_usage\"])\n",
    "print(calculate_cost_gpt4_turbo(res[\"total_usage\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "# import openai\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def information_retrieval_from_image(image_paths: List[str], query: List[str]):\n",
    "\n",
    "    # Getting the base64 string\n",
    "    encoded = []\n",
    "    if isinstance(image_paths, List):\n",
    "        for image_path in image_paths:\n",
    "            encoded.append(encode_image(image_path))\n",
    "    else:\n",
    "        raise ValueError(\"image_paths should be a list of image paths\")\n",
    "\n",
    "    logging.info(\"encoded the images\")\n",
    "    total_usage = {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}\n",
    "\n",
    "    conversation = [{\"role\": \"system\", \"content\": SIMPLE_INFO_RETRIEVAL_IMAGE_PROMPT}]\n",
    "    user_message = {\"role\": \"user\", \"content\": [f\"{query}\", *map(lambda x: {\"image\": x, \"resize\": 1024}, encoded)]}\n",
    "    conversation.append(user_message)\n",
    "\n",
    "    try:\n",
    "        response = creator(\n",
    "                **vision_model_defaults,\n",
    "                messages = conversation,\n",
    "                max_tokens = 3000\n",
    "                )\n",
    "        \n",
    "        response = response.model_dump()\n",
    "        text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    except:\n",
    "        # print(response.json())\n",
    "        raise ValueError(\"Error in the response\")\n",
    "\n",
    "    total_usage = add_dicts(total_usage, dict(response[\"usage\"]))\n",
    "\n",
    "    return json.loads(text), response[\"usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:encoded the images\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "dir_path = r\"C:\\Users\\DELL\\Documents\\Curate\\curate-v1\\core\\test_data\\mmh_english\"\n",
    "paths = sorted([os.path.join(dir_path, path) for path in os.listdir(dir_path)])\n",
    "\n",
    "t = information_retrieval_from_image(paths[:19], query = [\"teacher of qutubbudiin aibak\", \"slave dynasty years\", \"who died in 1236\", \"first to fix the prices of commodities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'queries': [{'query': '1', 'answers': ['Qutbuddin Aibak']}, {'query': '2', 'answers': ['1206-1526']}, {'query': '3', 'answers': ['Iltutmish']}, {'query': '4', 'answers': ['Alauddin Khilji']}]}\n"
     ]
    }
   ],
   "source": [
    "print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion_tokens': 117, 'prompt_tokens': 1835, 'total_tokens': 1952}\n"
     ]
    }
   ],
   "source": [
    "print(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curate-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
