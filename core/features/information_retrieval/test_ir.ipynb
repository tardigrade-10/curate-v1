{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information retrieval from text and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "curpath = os.getcwd()\n",
    "os.chdir(curpath.split(\"core\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "from prompts import SIMPLE_INFO_RETRIEVAL_TEXT_PROMPT, SIMPLE_INFO_RETRIEVAL_IMAGE_PROMPT\n",
    "from core.features.topic_segregation.utils import add_dicts, calculate_cost_gpt4_8k, calculate_cost_gpt4_turbo\n",
    "from core.features.provider import creator\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "text_model_defaults = {\"model\" : \"gpt-4-1106-preview\", \"temperature\" : 0.1, \"response_format\" : {\"type\": \"json_object\"}}\n",
    "vision_model_defaults = {\"model\" : \"gpt-4-vision-preview\", \"temperature\" : 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_from_raw_text(raw_text, sep = \"\\n\"):\n",
    "\n",
    "    if isinstance(raw_text, str):\n",
    "        raw_text_l = raw_text.splitlines()\n",
    "    elif isinstance(raw_text, list):\n",
    "        raw_text_l = raw_text\n",
    "    else:\n",
    "        raise ValueError(\"raw_text must be a string or a list\")\n",
    "\n",
    "    raw_text_list = []\n",
    "    for x in raw_text_l:\n",
    "        x = x.strip()\n",
    "        if x and len(x) > 2:\n",
    "            raw_text_list.append(x)\n",
    "\n",
    "    return list(set(raw_text_list))\n",
    "\n",
    "def chunk_raw_text_list(raw_text_list, max_len=3000):\n",
    "    \"\"\"\n",
    "    Splits a list of text strings into chunks where the total length of each chunk is \n",
    "    less than or equal to max_len.\n",
    "\n",
    "    :param raw_text_list: List of text strings to be chunked.\n",
    "    :param max_len: Maximum character length for each chunk.\n",
    "    :return: List of text chunks, each being a list of text strings.\n",
    "    \"\"\"\n",
    "    text_list = []\n",
    "    current_chunk = []\n",
    "    current_len = 0\n",
    "\n",
    "    for text in raw_text_list:\n",
    "        text_len = len(text)\n",
    "        \n",
    "        # Check if adding this text would exceed the max length\n",
    "        if current_len + text_len > max_len and current_chunk:\n",
    "            # Start a new chunk\n",
    "            text_list.append(current_chunk)\n",
    "            current_chunk = [text]\n",
    "            current_len = text_len\n",
    "        else:\n",
    "            # Add text to current chunk\n",
    "            current_chunk.append(text)\n",
    "            current_len += text_len\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        text_list.append(current_chunk)\n",
    "\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def text_path_to_chunk(path, max_len=20000, verbose=False):\n",
    "\n",
    "    with open(path, 'rb') as f:\n",
    "        _text = f.read().decode('utf-8')\n",
    "\n",
    "    total_chars = len(_text)\n",
    "\n",
    "    # convert raw_text to list and get unique values\n",
    "    unique_text_list = get_unique_from_raw_text(_text)\n",
    "\n",
    "    # chunking the list into sublists with max length of characters in a chunk is max_len\n",
    "    chunked_text_list = chunk_raw_text_list(unique_text_list, max_len=max_len)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Total chars:\", total_chars)\n",
    "        print(\"Unique Text Lists:\", len(unique_text_list))\n",
    "        print(\"Text List chunks:\", len(chunked_text_list))\n",
    "        for i in chunked_text_list:\n",
    "            t = \"\".join(i)\n",
    "            print(\"Chunk of size\", len(t), \"characters\")\n",
    "\n",
    "    return chunked_text_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def information_retrieval_from_text(raw_text: str, queries: List):\n",
    "\n",
    "    if len(raw_text) > 100000:\n",
    "        raise ValueError(\"Raw Text is too long. Should be less than 100000 characters\")\n",
    "    \n",
    "    # if len(queries) > 0:\n",
    "    #     queries = {i+1: q for i, q in enumerate(queries)}\n",
    "    # else:\n",
    "    #     raise ValueError(\"Atleast one query should be present\")\n",
    "\n",
    "    conversation = [{\"role\": \"system\", \"content\": SIMPLE_INFO_RETRIEVAL_TEXT_PROMPT}]\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    RAW_TEXT:\n",
    "\n",
    "    //raw_text//\n",
    "\n",
    "    {raw_text}\n",
    "\n",
    "    //raw_text//\n",
    "\n",
    "    Queries:\n",
    "\n",
    "    {queries}\n",
    "    \n",
    "    OUTPUT:\n",
    "    \"\"\"\n",
    "\n",
    "    user_message = {'role': 'user', \"content\": user_prompt}\n",
    "    conversation.append(user_message)\n",
    "\n",
    "    total_usage = {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}\n",
    "    response = creator(\n",
    "                    **text_model_defaults,\n",
    "                    messages = conversation,\n",
    "                    )\n",
    "    \n",
    "    total_usage = add_dicts(total_usage, dict(response.usage))\n",
    "\n",
    "    output = response.choices[0].message.content\n",
    "\n",
    "    return {\"output\": output, \"total_usage\": total_usage}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final batch processing function\n",
    "def ir_batch_process(queries: List, input_text: List[List]) -> [Dict, Dict]:\n",
    "\n",
    "    final_dict = {key:[] for key in list(queries.keys())}\n",
    "    # print(final_dict)\n",
    "    total_tokens = {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}\n",
    "\n",
    "    for raw_text_list in tqdm(input_text):\n",
    "        raw_text = \"\\n\".join(raw_text_list)\n",
    "\n",
    "        # print(raw_text)\n",
    "        result = information_retrieval_from_text(raw_text=raw_text, queries=queries)\n",
    "        output = result['output']\n",
    "        # print(output)\n",
    "        tokens = result['total_usage']\n",
    "\n",
    "        try:\n",
    "            json_out = json.loads(output)\n",
    "        except:\n",
    "            raise ValueError(\"Output is not a valid JSON\")\n",
    "        # print(json_out)\n",
    "        final_dict = add_dicts(final_dict, json_out)\n",
    "        total_tokens = add_dicts(total_tokens, tokens)\n",
    "\n",
    "    return final_dict, total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 215819\n",
      "Unique Text Lists: 108\n",
      "Text List chunks: 13\n",
      "Chunk of size 2810 characters\n",
      "Chunk of size 17233 characters\n",
      "Chunk of size 16352 characters\n",
      "Chunk of size 19999 characters\n",
      "Chunk of size 13598 characters\n",
      "Chunk of size 15513 characters\n",
      "Chunk of size 15487 characters\n",
      "Chunk of size 19877 characters\n",
      "Chunk of size 18857 characters\n",
      "Chunk of size 17578 characters\n",
      "Chunk of size 16973 characters\n",
      "Chunk of size 17918 characters\n",
      "Chunk of size 19369 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:13<00:00,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 45694, 'total_tokens': 47322, 'completion_tokens': 1628}\n",
      "0.50578\n",
      "Query: All College Names\n",
      "['universities', 'NC State College of Education', 'The Academy School', 'Applecross Senior High School', 'Ashdale Secondary College', 'Manea Senior College', 'Western Australian College of Agriculture Cunderdin', 'IIT Kanpur', 'IIS Kanpur', 'Singapore Institute of Management Global Education', 'Garodia International College', 'Jagannath Institute of Management School', 'University at Buffalo, State University of New York', 'University of London', 'University of Birmingham', 'RMIT', 'University of Wollongong', 'Sydney University', 'Indian Institute of Technology Kanpur', 'Indian Institute of Skills', 'Antai College of economics and management', 'Shanghai Jiao Tong University', 'New Saraswati House', 'USC', 'Northern Virginia Community College', 'George Mason University', 'Irvine Valley College', 'California State University-Fullerton', 'Heartland Community College', 'Illinois State University', 'Tri-County Technical College', 'Clemson University', 'Marion Military Institute', 'NorthWest Arkansas Community College', 'Chandler-Gilbert Community College', 'Colorado Northwestern Community College', 'Norwalk Community College', 'Delaware Technical Community College-Terry', 'Tallahassee Community College', 'South Georgia State College', 'Kapiolani Community College', 'Ellsworth Community College', 'College of Southern Idaho', 'William Rainey Harper College', 'Vincennes University', 'Barton County Community College', 'Hopkinsville Community College', 'Louisiana State University-Eunice', 'Massachusetts Bay Community College', 'Montgomery College', 'Kennebec Valley Community College', 'Muskegon Community College', 'Normandale Community College', 'St Charles Community College', 'Mississippi Delta Community College', 'Dawson Community College', 'Coastal Carolina Community College', 'Dakota College at Bottineau', 'Mid-Plains Community College', \"NHTI-Concord's Community College\", 'County College of Morris', 'New Mexico Military Institute', 'Western Nevada College', 'Stella and Charles Guttman Community College', 'Columbus State Community College', 'Northeastern Oklahoma A&M College', 'Clackamas Community College', 'Bucks County Community College', 'Community College of Rhode Island', 'University of South Carolina-Sumter', 'Western Dakota Technical Institute', 'Motlow State Community College', 'The University of Texas at Brownsville', 'Snow College', 'Richard Bland College of the College of William and Mary', 'Community College of Vermont', 'Bellevue College', 'University of Wisconsin Colleges', 'Southern West Virginia Community and Technical College', 'Northwest College', 'Auburn University', 'University of Arkansas', 'University of Arizona', 'California Polytechnic State University-San Luis Obispo', 'University of Northern Colorado', 'University of Connecticut', 'Wilmington University', 'University of Miami', 'University of Georgia', 'University of Hawaii at Manoa', 'Mount Mercy University', 'University of Idaho', 'University of Illinois at Urbana-Champaign', 'Purdue University-Main Campus', 'Kansas State University', 'University of Kentucky', 'Louisiana State University and A&M College', 'University of Massachusetts-Lowell', 'University of Maryland-College Park', 'University of Southern Maine', 'University of Michigan-Ann Arbor', 'University of Minnesota-Twin Cities', 'Missouri University of Science and Technology', 'Mississippi University for Women', 'University of North Carolina at Chapel Hill', 'North Dakota State University-Main Campus', 'University of Nebraska-Lincoln', 'University of New Hampshire-Main Campus', 'The College of New Jersey', 'University of New Mexico-Main Campus', 'University of Nevada-Reno', 'Saint John Fisher College', 'Ohio State University-Main Campus', 'Oklahoma State University-Main Campus', 'University of Oregon', 'Thomas Jefferson University', 'University of Rhode Island', 'Tennessee Technological University', 'Texas A & M University-College Station', 'University of Utah', 'Virginia Polytechnic Institute and State University', 'Western Washington University', 'University of Wisconsin-Madison', 'Fairmont State University', 'University of Wyoming', 'Southern Union State Community College', 'Arizona State University Campus Immersion', 'Pikes Peak State College', 'University of Colorado Colorado Springs', 'Manchester Community College', 'Central Connecticut State University', 'East Georgia State College', 'Georgia Southern University', 'College of Western Idaho', 'Boise State University', 'Ivy Tech Community College', 'Indiana University-Purdue University-Indianapolis', 'Hawkeye Community College', 'University of Northern Iowa', 'Butler Community College', 'Wichita State University', 'Southcentral Kentucky Community and Technical College', 'Western Kentucky University', 'South Louisiana Community College', 'University of Louisiana at Lafayette', 'Southern Maine Community College', 'Wor-Wic Community College', 'Salisbury University', 'Middlesex Community College', 'Kalamazoo Valley Community College', 'Western Michigan University', 'Rochester Community and Technical College', 'Winona State University', 'Jones County Junior College', 'University of Southern Mississippi', 'Missouri State University-West Plains', 'Missouri State University-Springfield', 'Northeast Community College', 'Wayne State College', 'Truckee Meadows Community College', 'Middlesex College', 'Rutgers University-New Brunswick', 'Central New Mexico Community College', 'CUNY Kingsborough Community College', 'CUNY Brooklyn College', 'Central Piedmont Community College', 'University of North Carolina at Charlotte', 'Northern Oklahoma College', 'Portland Community College', 'Portland State University', 'Seattle Central College', 'University of Washington-Seattle Campus', 'Northeast Wisconsin Technical College', 'University of Wisconsin-Green Bay', 'All India Institute of Medical Sciences', 'Sydney University', 'IESE Business School', 'USC Rossier School of Education', 'USC Suzanne Dworak-Peck School of Social Work', 'USC Iovine and Young Academy', 'KL Deemed to be University', 'Sydney University', 'De La Salle University', 'Australian Catholic University', 'IIT Indore', 'IIM Indore', 'MANIT', 'IIFM', 'AIIMS Bhopal', 'NLIU Bhopal']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunked_text_list = text_path_to_chunk(path=r\"C:\\Users\\DELL\\Documents\\Curate\\curate-v1\\core\\features\\information_retrieval\\test_news1.txt\", verbose=True)\n",
    "queries = [\"All College Names\"]\n",
    "indexed_queries = {str(i+1): query for i, query in enumerate(queries)}\n",
    "res, tokens = ir_batch_process(queries=indexed_queries, input_text=chunked_text_list)\n",
    "\n",
    "print(tokens)\n",
    "print(calculate_cost_gpt4_turbo(tokens))\n",
    "for k, query in indexed_queries.items():\n",
    "    print(f\"Query: {query}\")\n",
    "    print(res[k])\n",
    "    print()\n",
    "\n",
    "# for i in res[queries[0]]:\n",
    "#     print(text_dict[int(i)])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n"
     ]
    }
   ],
   "source": [
    "{'prompt_tokens': 45694, 'total_tokens': 47322, 'completion_tokens': 1628}\n",
    "0.50578"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "# import openai\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def information_retrieval_from_image(image_paths: List[str], query: List[str]):\n",
    "\n",
    "    # Getting the base64 string\n",
    "    encoded = []\n",
    "    if isinstance(image_paths, List):\n",
    "        for image_path in image_paths:\n",
    "            encoded.append(encode_image(image_path))\n",
    "    else:\n",
    "        raise ValueError(\"image_paths should be a list of image paths\")\n",
    "\n",
    "    # logging.info(\"encoded the images\")\n",
    "    total_usage = {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}\n",
    "\n",
    "    conversation = [{\"role\": \"system\", \"content\": SIMPLE_INFO_RETRIEVAL_IMAGE_PROMPT}]\n",
    "    user_message = {\"role\": \"user\", \"content\": [f\"{query}\", *map(lambda x: {\"image\": x, \"resize\": 1024}, encoded)]}\n",
    "    conversation.append(user_message)\n",
    "\n",
    "    try:\n",
    "        response = creator(\n",
    "                **vision_model_defaults,\n",
    "                messages = conversation,\n",
    "                max_tokens = 3000\n",
    "                )\n",
    "        \n",
    "        response = response.model_dump()\n",
    "        text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    except:\n",
    "        # print(response.json())\n",
    "        raise ValueError(\"Error in the response\")\n",
    "\n",
    "    total_usage = add_dicts(total_usage, dict(response[\"usage\"]))\n",
    "\n",
    "    return json.loads(text), response[\"usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:encoded the images\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "dir_path = r\"C:\\Users\\DELL\\Documents\\Curate\\curate-v1\\core\\test_data\\mmh_english\"\n",
    "paths = sorted([os.path.join(dir_path, path) for path in os.listdir(dir_path)])\n",
    "\n",
    "t = information_retrieval_from_image(paths[:19], query = [\"teacher of qutubbudiin aibak\", \"slave dynasty years\", \"who died in 1236\", \"first to fix the prices of commodities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': ['Qutbuddin Aibak'], '2': ['1206-1526'], '3': ['Iltutmish'], '4': ['Alauddin Khilji']}\n"
     ]
    }
   ],
   "source": [
    "print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion_tokens': 66, 'prompt_tokens': 1808, 'total_tokens': 1874}\n"
     ]
    }
   ],
   "source": [
    "print(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curate-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
