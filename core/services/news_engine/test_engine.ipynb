{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "curpath = os.getcwd()\n",
    "os.chdir(curpath.split(\"core\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "import urllib3\n",
    "from xml.dom.minidom import parseString\n",
    "from core.services.provider import creator, creator_defaults\n",
    "\n",
    "# from prompts import SIMPLE_TS_PROMPTS, SIMPLE_TS_WITH_REF_PROMPTS\n",
    "# from core.services.topic_segregation.utils import add_dicts, calculate_cost_gpt4_8k\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator(**creator_defaults, messages=[{'role': 'user', 'content': 'Hello, world!'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_news_results(term:str, count: int):\n",
    "    http = urllib3.PoolManager()\n",
    "    results = []\n",
    "    response = http.request('GET', 'http://news.google.com/news?q=%s&lang=en&output=rss' % term)\n",
    "    obj = parseString(response.data.decode('utf-8'))\n",
    "    items = obj.getElementsByTagName('item')  # Get each item\n",
    "    for item in items[:count]:\n",
    "        title, link = '', ''\n",
    "        for node in item.childNodes:\n",
    "            if node.nodeName == 'title':\n",
    "                title = node.childNodes[0].data\n",
    "            elif node.nodeName == 'link':\n",
    "                link = node.childNodes[0].data\n",
    "        results.append((title, link))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_google_news_results('Rajasthan%20latest%20news', 100)\n",
    "titles = []\n",
    "links = []\n",
    "for title, link in results:\n",
    "    titles.append(title)\n",
    "    links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://news.google.com/rss/articles/CBMiSmh0dHBzOi8vdGhld2lyZS5pbi9wb2xpdGljcy9yYWphc3RoYW4tY29uZ3Jlc3MtbWFkaHlhLXByYWRlc2gtY2hoYXR0aXNnYXJo0gFYaHR0cHM6Ly9tLnRoZXdpcmUuaW4vYXJ0aWNsZS9wb2xpdGljcy9yYWphc3RoYW4tY29uZ3Jlc3MtbWFkaHlhLXByYWRlc2gtY2hoYXR0aXNnYXJoL2FtcA?oc=5'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Win or Lose: Rajasthan Shows The Way Forward for the Congress - The Wire'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_txt = \"\\n\".join(titles)\n",
    "\n",
    "with open ('news_data_raj_en.txt', \"wb\") as f:\n",
    "    f.write(title_txt.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scrape at 2023-11-10 13:51:05.010343\n",
      "Scrape complete at 2023-11-10 13:51:10.337551\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "class InformationScraper:\n",
    "    def __init__(self, queries, frequency='daily'):\n",
    "        self.queries = queries\n",
    "        self.frequency = frequency\n",
    "        self.results = []\n",
    "\n",
    "    def fetch_news(self, query):\n",
    "        # Replace this URL with the actual news site or API you're planning to scrape\n",
    "        url = f'https://news.google.com/search?q={query}'\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Failed to retrieve news for query: {query}\")\n",
    "            return None\n",
    "\n",
    "    def parse_news(self, html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Modify the selectors based on the actual structure of the news site\n",
    "        articles = soup.find_all('article')\n",
    "        news_items = []\n",
    "        for article in articles:\n",
    "            title = article.find('h3').get_text() if article.find('h3') else 'No Title'\n",
    "            link = article.find('a', href=True)['href'] if article.find('a', href=True) else 'No Link'\n",
    "            news_items.append({'title': title, 'link': link})\n",
    "        return news_items\n",
    "\n",
    "    def scrape(self):\n",
    "        for query in self.queries:\n",
    "            html_content = self.fetch_news(query)\n",
    "            if html_content:\n",
    "                news_items = self.parse_news(html_content)\n",
    "                self.results.extend(news_items)\n",
    "\n",
    "    def run(self):\n",
    "        if self.frequency == 'daily':\n",
    "            while True:\n",
    "                print(f\"Starting scrape at {datetime.now()}\")\n",
    "                self.scrape()\n",
    "                print(f\"Scrape complete at {datetime.now()}\")\n",
    "                break\n",
    "                time.sleep(86400)  # Sleep for 1 day\n",
    "        elif self.frequency == 'hourly':\n",
    "            while True:\n",
    "                print(f\"Starting scrape at {datetime.now()}\")\n",
    "                self.scrape()\n",
    "                print(f\"Scrape complete at {datetime.now()}\")\n",
    "                break\n",
    "                time.sleep(3600)  # Sleep for 1 hour\n",
    "        # Add more frequency options as needed\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "\n",
    "# Example usage:\n",
    "queries = ['education', 'e-learning', 'online courses']\n",
    "scraper = InformationScraper(queries, frequency='daily')\n",
    "scraper.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://news.google.com/articles/CBMihgFodHRwczovL3d3dy5idXNpbmVzcy1zdGFuZGFyZC5jb20vZWNvbm9teS9uZXdzL3RheC13aW5kZmFsbC1naXZlcy1tb2RpLWdvdnQtc2NvcGUtdG8tc3BlbmQtbW9yZS1vbi13ZWxmYXJlLXNjaGVtZXMtMTIzMTEwODAwNTEzXzEuaHRtbNIBigFodHRwczovL3d3dy5idXNpbmVzcy1zdGFuZGFyZC5jb20vYW1wL2Vjb25vbXkvbmV3cy90YXgtd2luZGZhbGwtZ2l2ZXMtbW9kaS1nb3Z0LXNjb3BlLXRvLXNwZW5kLW1vcmUtb24td2VsZmFyZS1zY2hlbWVzLTEyMzExMDgwMDUxM18xLmh0bWw?hl=en-IN&gl=IN&ceid=IN%3Aen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "gnews = \"https://news.google.com\"\n",
    "\n",
    "for res in scraper.results:\n",
    "    links.append(gnews + res['link'][1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, TemplateString\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def scraper(urls):\n",
    "    \n",
    "    results = []\n",
    "    for url in tqdm(urls):\n",
    "\n",
    "        # Send a GET request to the URL\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            time.sleep(0.5)\n",
    "        except:\n",
    "            print(f'Failed to get {url}')\n",
    "            continue\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the content of the page using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "            for data in soup(['head', 'header', 'style', 'script', 'g', 'polygon', 'path', 'svg', 'option', 'footer']):\n",
    "                # Remove tags\n",
    "                data.decompose()\n",
    "\n",
    "            article_text = ' '.join(paragraph.text.strip() for paragraph in soup.find_all('p'))\n",
    "            \n",
    "            # Extract the headline\n",
    "            headline = soup.find('h1')\n",
    "            if headline:\n",
    "                headline = headline.text.strip()\n",
    "            else:\n",
    "                headline = ''\n",
    "\n",
    "            result = headline + '\\n' + article_text\n",
    "            results.append(result)\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "            print(f\"Error in URL: {url}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:14<01:16,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 401\n",
      "Error in URL: https://news.google.com/articles/CBMigwFodHRwczovL3d3dy5oaW5kdXN0YW50aW1lcy5jb20vbGlmZXN0eWxlL2FydC1jdWx0dXJlL25hdGlvbmFsLWVkdWNhdGlvbi1kYXktMjAyMy1kYXRlLWhpc3RvcnktYW5kLXNpZ25pZmljYW5jZS0xMDE2OTk1MjA5MzMyMzAuaHRtbNIBhwFodHRwczovL3d3dy5oaW5kdXN0YW50aW1lcy5jb20vbGlmZXN0eWxlL2FydC1jdWx0dXJlL25hdGlvbmFsLWVkdWNhdGlvbi1kYXktMjAyMy1kYXRlLWhpc3RvcnktYW5kLXNpZ25pZmljYW5jZS0xMDE2OTk1MjA5MzMyMzAtYW1wLmh0bWw?hl=en-IN&gl=IN&ceid=IN%3Aen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [00:33<00:50,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 401\n",
      "Error in URL: https://news.google.com/articles/CBMiiwFodHRwczovL3d3dy5oaW5kdXN0YW50aW1lcy5jb20vZWR1Y2F0aW9uL25ld3MvZHBzcnUtaW5rcy1tb3Utd2l0aC1ha3Vtcy1kcnVncy10by1lbmhhbmNlLXBoYXJtYS1lZHVjYXRpb24tYW5kLXJlc2VhcmNoLTEwMTY5OTAxMTY5NDU5Mi5odG1s0gGPAWh0dHBzOi8vd3d3LmhpbmR1c3RhbnRpbWVzLmNvbS9lZHVjYXRpb24vbmV3cy9kcHNydS1pbmtzLW1vdS13aXRoLWFrdW1zLWRydWdzLXRvLWVuaGFuY2UtcGhhcm1hLWVkdWNhdGlvbi1hbmQtcmVzZWFyY2gtMTAxNjk5MDExNjk0NTkyLWFtcC5odG1s?hl=en-IN&gl=IN&ceid=IN%3Aen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [01:07<00:38,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 403\n",
      "Error in URL: https://news.google.com/articles/CBMiW2h0dHBzOi8vd3d3LmFwcy5lZHUvbmV3cy9uZXdzLWZyb20tMjAyMy0yMDI0L2Fwcy1ib2FyZC1vZi1lZHVjYXRpb24tY29tbXVuaXR5LWNvbnZlcnNhdGlvbnPSAQA?hl=en-IN&gl=IN&ceid=IN%3Aen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [01:13<00:29,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 400\n",
      "Error in URL: https://news.google.com/articles/CBMiR2h0dHBzOi8vbWVkaWNhbHhwcmVzcy5jb20vbmV3cy8yMDIzLTExLWxpbmtlZC1oZWFydC1oZWFsdGgtZGVjYWRlcy5odG1s0gFGaHR0cHM6Ly9tZWRpY2FseHByZXNzLmNvbS9uZXdzLzIwMjMtMTEtbGlua2VkLWhlYXJ0LWhlYWx0aC1kZWNhZGVzLmFtcA?hl=en-IN&gl=IN&ceid=IN%3Aen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [01:16<00:27,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 403\n",
      "Error in URL: https://news.google.com/articles/CBMiLmh0dHBzOi8vc3RhdGVtZW50cy5xbGQuZ292LmF1L3N0YXRlbWVudHMvOTkxMjnSAQA?hl=en-IN&gl=IN&ceid=IN%3Aen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [01:44<01:13,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get https://news.google.com/articles/CBMiZ2h0dHBzOi8vd3d3Lm1vbnRldmFsbG8uZWR1L3VtLXRvLWhvc3QtNTFzdC1hbm51YWwtYWxhYmFtYS1taXNzaXNzaXBwaS1zb2NpYWwtd29yay1lZHVjYXRpb24tY29uZmVyZW5jZS_SAQA?hl=en-IN&gl=IN&ceid=IN%3Aen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:11<00:00,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 403\n",
      "Error in URL: https://news.google.com/articles/CBMiU2h0dHBzOi8vd3d3LnRoZWxhbmNldC5jb20vam91cm5hbHMvbGFuY2V0L2FydGljbGUvUElJUzAxNDAtNjczNigyMykwMjQ2Mi01L2Z1bGx0ZXh00gEA?hl=en-IN&gl=IN&ceid=IN%3Aen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = scraper(links[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)\n",
    "\n",
    "with open(\"test_news.txt\", 'wb') as f:\n",
    "    for result in results:\n",
    "        f.write(result.encode('utf-8').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, TemplateString\n",
    "\n",
    "# def scraper(url):\n",
    "    # The URL of the article to scrape\n",
    "url = \"https://me.mashable.com/culture/34493/northern-lights-paint-bulgarias-sky-in-eerie-red-hue-see-pics\"\n",
    "url = \"https://www.ndtv.com/india-news/cbi-inquiry-against-mahua-moitra-ordered-by-anti-corruption-panel-bjp-mp-4557198\"\n",
    "url = \"https://thewire.in/security/india-armed-forces-politicisation-by-default-or-design\"\n",
    "url = \"https://www.washingtonpost.com/politics/2023/11/07/abortion-ohio-kentucky-virginia-elecitons/\"\n",
    "url = \"https://www.theinformation.com/articles/american-ai-startups-raise-money-from-top-chinese-vc-firms-including-sequoia-capital-china\"\n",
    "url = \"https://www.nytimes.com/2023/11/08/business/energy-environment/electric-vehicles-biden.html\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # soup = BeautifulSoup(html, \"html.parser\")\n",
    " \n",
    "    for data in soup(['head', 'header', 'style', 'script', 'g', 'polygon', 'path', 'svg', 'option', 'footer']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "    # temp = TemplateString(response.content)\n",
    "    print(soup)\n",
    "    print()\n",
    "\n",
    "    article_text = ' '.join(paragraph.text.strip() for paragraph in soup.find_all('p'))\n",
    "\n",
    "    # print(article_text)\n",
    "    # print(len(soup))\n",
    "    # print(temp)\n",
    "    # print(soup)\n",
    "\n",
    "    # soup.find_all('script')\n",
    "    # soup.find_all()\n",
    "    \n",
    "    # Extract the headline\n",
    "    headline = soup.find('h1')\n",
    "    if headline:\n",
    "        headline = headline.text.strip()\n",
    "    \n",
    "#     # Try to find the container of the article content\n",
    "#     # The class or tag here is hypothetical and should be replaced with the actual class or tag that contains the article text.\n",
    "#     article_content = soup.find('article') \n",
    "#     # or soup.find('main')  # Adjust the class or tag as needed\n",
    "    \n",
    "#     # Check if the article content was found\n",
    "    # if article_content:\n",
    "    #     paragraphs = article_content.find_all('p')\n",
    "    #     article_text = ' '.join(paragraph.text.strip() for paragraph in paragraphs)\n",
    "    # else:\n",
    "    #     article_text = \"Could not find the article content.\"\n",
    "    \n",
    "    # Print the extracted information\n",
    "    print(\"Headline:\", headline)\n",
    "    print(\"Article Text:\", article_text)\n",
    "# else:\n",
    "#     print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: CBI Probe Against Mahua Moitra Ordered By Anti-Graft Panel, Claims BJP MP\n",
      "Article Text: Anti-corruption body Lokpal has ordered a probe by the Central Bureau of Investigation into the corruption allegations against Trinamool Congress MP Mahua Moitra, the BJP's Nishikant Dubey posted today on X, formerly Twitter. Ms Moitra has been embroiled in cash for query allegations levelled by Mr Dubey, which is under the lens of the Parliamentary Ethics Committee. In a letter to Lok Sabha Speaker Om Birla, he had demanded her immediate suspension from parliament. \"On basis of my complaint, Lokpal has ordered a CBI probe into the Mahua Moitra's corruption that compromises national security,\" read a rough translation of BJP MP's post. Nishikant Dubey has alleged that Ms Moitra accepted cash from businessman Darshan Hiranandani to ask questions in parliament on his behalf to target Prime Minister Narendra Modi and business rival Adani Group.  He has also claimed that Ms Moitra, by sharing her parliamentary login with the businessman, has compromised national security. In an explosive affidavit, Darshan Hiranandani has admitted to the login sharing, but bypassed the cash for query issue, saying he had given Ms Moitra gifts that she had demanded. There are allegations that Ms Moitra's login has been used multiple times from Dubai, where Mr Hiranandani currently resides. Mahua Moitra has accepted that she shared her login, but claimed the dos and don'ts of that matter has not been shared with the MPs. Ahead of her hearing with the Ethics Committee, she had written them a letter, mentioning that there is no regulation governing sharing of login and password. \"Why were these rules not shared with MPs,\" she had questioned in the letter. The MP had stormed out the meeting, which took place last week, accusing the committee of subjecting her to \"proverbial vastraharan (stripping)\" in a letter to the Speaker. PromotedListen to the latest songs, only on JioSaavn.com The Ethics panel chief -- BJP MP Vinod Kumar Sonkar -- has focussed on Ms Moitra's personal relationship with Darshan Hiranandani. There were questions too on Jai Anant Dehadrai, on whose complaint to the Central Bureau of Investigation the entire case rests. The Committee has accused Ms Moitra of non-cooperation. \"Mahua Moitra did not cooperate with the committee and the investigation. The Opposition members also made allegations in anger and suddenly walked out of the meeting to avoid answering more questions,\" Mr Sonkar said. Track Latest News Live on NDTV.com and get news updates from India  and around the world. Watch Live News: Follow Us:\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.ndtv.com/india-news/cbi-inquiry-against-mahua-moitra-ordered-by-anti-corruption-panel-bjp-mp-4557198\"\n",
    "\n",
    "scraper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curate-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
